{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a starting point for metadata about the glasses.\n",
    "\n",
    "- (1) we need estimates of GoPro Timestamp at start\n",
    "- (2) we need to reduce the filesize of the video CSV from openface to something workable\n",
    "- (3) we need to keep track of where the files are, etc.\n",
    "- (4) we will save all of this to a data structure that we can load in other places for convenience\n",
    "\n",
    "----\n",
    "\n",
    "First step is to run:\n",
    "\n",
    "(1)  `ls *.MP4 | sed \"s/^/file '/g; s/$/'/g\" > filelist.txt` followed by `ffmpeg -f concat -safe 0 -i filelist.txt -c copy video.mp4` to concatentate the gopro videos into one.\n",
    "\n",
    "(2)  then we run the openface feature extractor with `FeatureExtraction.exe -f \"E:\\data_cap_val_2\\patrick_1\\GoPro\\video.mp4\" -2Dfp -3Dfp -pdmparams -pose -aus -gaze`.  This will give all kinds of features from openface, timestamped in secs relative to start of video, one row per frame.\n",
    "\n",
    "(3) Finally we use `python3 run_blink_extractor_threaded.py --video video.mp4` to get an '\\_eyeratio\\_final.csv' file. This file has relative timestamps from the start of the video in seconds, as well as an eye ratio that we can use to calculate blinks based on a threshold, one row per frame.  The script and how to process it are located in the Video Blink Extraction directory.\n",
    "\n",
    "----\n",
    "\n",
    "We will copy over the files to this folder 'cleaned_data'.  The folder contains:\n",
    "- `captivateFiltered.json` (the full MongoDB dump, as a JSON file).\n",
    "- `metadata.p` (a file we create that tells us basic data that we input; i.e. what day each test was, who it was, which folder it corresponds to, an estimated timestamp for the gopro video starting, how large of an error we expect in this estimate, an estimate of gopro duration, an estimate of glasses data session start times and durations).  This file will be generated with this script.  Helpers to estimate gopro start time are included here.\n",
    "- folders for each user test by day, i.e. `david_1, patrick_1, david_2, juliana_1, etc`\n",
    "- - each folder will have a `video_eyeratio_final.csv` file from the blink script run on the video\n",
    "- - each folder will have a `video.csv` file from openface run on the video (optionally)\n",
    "- - each folder will have a `video_min.csv` file with just the most important features from `video.csv`, generated with this script.  GoPro video was recorded at 60 Hz (FPS).\n",
    "- - each folder will have a `glasses_sessions.p` file, which includes the relevant sorted data as continuous sessions from the glasses (at 10Hz, blink data is 1kHz).  This file will generate that data.\n",
    "\n",
    "----\n",
    "To use this, download the mongoDB json file and put it in this folder.  Generate a folder for the session with `video.csv` and `video_eyeratio_final.csv` manually.  Add in metadata below (a gopro estimated timestamp, date of test, session name, etc) and run through this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime , timezone\n",
    "import time\n",
    "import numpy as np\n",
    "import codecs\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating a GO-PRO timestamp\n",
    "\n",
    "GoPro videos are all 11m47s long at 60 fps.  There is no easy way to get a timestamp for the video-- the create date and the last modified date that should come as embedded metadata don't seem to be timestamped accurately.  \n",
    "\n",
    "Based on what we've seen, it looks like the best estimate of the start time of the video is the 'date modified' of the first gopro video file minus the 11m47s duration of the video.  Here's an example from Juliana; based on when it looks like the glasses turned on in the video and our first glasses packet, we expect recording started at around 10:21:40am on 04/09:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_ms_to_string(timestamp):\n",
    "    local_tz = datetime.now().astimezone().tzinfo\n",
    "    return datetime.fromtimestamp(timestamp/1000, tz=local_tz).strftime('%m/%d/%y %I:%M:%S%p %Z')\n",
    "\n",
    "def string_to_timestamp_ms(datestring):\n",
    "    return datetime.strptime(datestring, '%m/%d/%y %I:%M:%S%p %Z').timestamp()*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified time: 04/09/21 10:33:20AM EDT\n",
      "create time: 04/14/21 05:49:40PM EDT\n",
      "modified time - 11m47s: 04/09/21 10:21:33AM EDT\n"
     ]
    }
   ],
   "source": [
    "fname = pathlib.Path('/Volumes/ExtDrive_(ResEnv)/data_cap_val_2/juliana_1/GoPro/GH010082.MP4')\n",
    "\n",
    "modified_time = fname.stat().st_mtime\n",
    "create_time = fname.stat().st_ctime\n",
    "\n",
    "print('modified time:', timestamp_ms_to_string(modified_time*1000))\n",
    "print('create time:', timestamp_ms_to_string(create_time*1000))\n",
    "print('modified time - 11m47s:', timestamp_ms_to_string(modified_time*1000 - (11*60+47)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use the below to find a gopro start timestamp for each participant based on the first video file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06/21 04:48:22PM EDT\n"
     ]
    }
   ],
   "source": [
    "#apply the above to new participants:\n",
    "fname = pathlib.Path('/Volumes/ExtDrive_(ResEnv)/data_cap_val_2/david_3/GH010078.MP4')\n",
    "print(timestamp_ms_to_string(fname.stat().st_mtime*1000 - (11*60+47)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "### DAVID_2 (3/25, 2p - 8:30p)\n",
    "\n",
    "*No GoPro video to timestamp.* Glasses use appears to start at 44min12sec into 6hr28min45sec video, so about 5hr45min of useful data.  We see 5hr51 of glasses data, so this matches.  The timestamp of first glasses packet is 03/25/21 02:25:13PM EDT, so we'll assume the video starts at **03/25/21 01:41:01PM EDT**.\n",
    "\n",
    "### PATRICK_1 (3/29, 3:30p - 8p)\n",
    "\n",
    "*No GoPro video to timestamp.*\n",
    "\n",
    "### PATRICK_2 (4/1, 2:45p - 8p)\n",
    "\n",
    "*No GoPro video to timestamp.*\n",
    "\n",
    "### BEATA_1 (4/2, 12:15p - 9:15p) \n",
    "\n",
    "*No GoPro video to timestamp.*  GoPro starts within 12:16pm min based on apple watch in frame, time guess 41:09, focus guess 41:26.  we'll assume the video starts at **04/02/21 12:16:30PM EDT**.\n",
    "\n",
    "### DAVID_3 (4/6, 5:30p - 11:30p)\n",
    "\n",
    "GoPro reset!! Seems we have 40 min of video GH010078.mp4 and GH020078.mp4 before a reset, which then changes the names starting at GH010079.mp4, GH020079.mp4....  The starting time of the first ~40 min video is **04/06/21 04:48:22PM EDT**, while the main section is **04/06/21 05:28:57PM EDT**.  We can split this into 2.\n",
    "\n",
    "### IRMANDY_1 (4/8, 5:30p - 12p)\n",
    " \n",
    "only 11 minutes of video BOOOOOOO.  Starts at **04/08/21 05:30:56PM EDT**.\n",
    "\n",
    "### JULIANA_1 (4/9, 10a - 3p)\n",
    "\n",
    "GoPro starts before glasses, seems to be about ~20-30s before.  GoPro Video 1 gives a date created of 04/10/21 00:58, and a date modified of 04/09/21 10:33.  Juliana glasses data starts at 04/09/21 10:22:05AM EDT, so we expect a start time of 10:21:40 or so.  Based on the create time of the video, using the example above, we get a start time of **04/09/21 10:21:33AM EDT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "now that we've gotten our gopro timestamps, and we've create folders in this folder with all of the raw openface and eyeratio data and the latest mongo instance, let's add our gopro timestamps below and process the data into useable chunks.  We start with this structure, which we hand populate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata = {\n",
    "'david_2'  : {'date': '03/25', 'vid_start': '03/25/21 01:41:01PM EDT', 'start_error_sec':60},      \n",
    "'patrick_1': {'date': '03/29', 'vid_start': '', 'start_error_sec':60},\n",
    "'patrick_2': {'date': '04/01', 'vid_start': '', 'start_error_sec':60},\n",
    "'beata_1'  : {'date': '04/02', 'vid_start': '04/02/21 12:16:30PM EDT', 'start_error_sec':60},\n",
    "'david_3a' : {'date': '04/06', 'vid_start': '04/06/21 04:48:22PM EDT', 'start_error_sec':15},\n",
    "'david_3b' : {'date': '04/06', 'vid_start': '04/06/21 05:28:57PM EDT', 'start_error_sec':15},\n",
    "'irmandy_1': {'date': '04/08', 'vid_start': '04/08/21 05:30:56PM EDT', 'start_error_sec':15},\n",
    "'juliana_1': {'date': '04/09', 'vid_start': '04/09/21 10:21:33AM EDT', 'start_error_sec':15}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Save Glasses Data as Sessions with Each User\n",
    "\n",
    "let's first load in our glasses data and process them into packets.  We can store relevant\n",
    "sessions in the folder that corresponds the the day above, and store metadata about session\n",
    "start and end time.  This will improve loading time, as the full dataset is GB large (small enough to fit in RAM, but large enough to be a huge pain to load frequently).\n",
    "\n",
    "If this gets too out of hand to reprocess for each participant, we should start a new 'batch' in a new folder.  This is the only hefty operation for each participant that is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 sessions found. (98 short sessions filtered.)\n",
      "\n",
      " 03/24/21 ------\n",
      "\t 03/24/21 04:10:28PM EDT to 03/24/21 04:22:03PM EDT \t duration=00:11\n",
      "\t 03/24/21 04:25:50PM EDT to 03/24/21 06:47:50PM EDT \t duration=02:21\n",
      "\n",
      "\t total time spent: 02:33\n",
      "\n",
      " 03/25/21 ------\n",
      "\t 03/25/21 02:25:13PM EDT to 03/25/21 05:55:40PM EDT \t duration=03:30\n",
      "\t 03/25/21 06:11:11PM EDT to 03/25/21 08:32:30PM EDT \t duration=02:21\n",
      "\n",
      "\t total time spent: 05:51\n",
      "\n",
      " 03/29/21 ------\n",
      "\t 03/29/21 03:28:19PM EDT to 03/29/21 04:07:45PM EDT \t duration=00:39\n",
      "\t 03/29/21 04:10:38PM EDT to 03/29/21 05:48:17PM EDT \t duration=01:37\n",
      "\t 03/29/21 05:51:01PM EDT to 03/29/21 06:49:15PM EDT \t duration=00:58\n",
      "\t 03/29/21 06:49:41PM EDT to 03/29/21 07:27:35PM EDT \t duration=00:37\n",
      "\t 03/29/21 07:29:10PM EDT to 03/29/21 08:01:04PM EDT \t duration=00:31\n",
      "\n",
      "\t total time spent: 04:25\n",
      "\n",
      " 04/01/21 ------\n",
      "\t 04/01/21 02:35:30PM EDT to 04/01/21 02:45:05PM EDT \t duration=00:09\n",
      "\t 04/01/21 03:07:10PM EDT to 04/01/21 03:23:42PM EDT \t duration=00:16\n",
      "\t 04/01/21 03:24:54PM EDT to 04/01/21 03:44:40PM EDT \t duration=00:19\n",
      "\t 04/01/21 04:02:56PM EDT to 04/01/21 05:12:18PM EDT \t duration=01:09\n",
      "\t 04/01/21 05:13:58PM EDT to 04/01/21 05:47:34PM EDT \t duration=00:33\n",
      "\t 04/01/21 05:47:39PM EDT to 04/01/21 07:38:31PM EDT \t duration=01:50\n",
      "\t 04/01/21 07:39:42PM EDT to 04/01/21 08:04:11PM EDT \t duration=00:24\n",
      "\n",
      "\t total time spent: 04:44\n",
      "\n",
      " 04/02/21 ------\n",
      "\t 04/02/21 12:13:25PM EDT to 04/02/21 04:39:00PM EDT \t duration=04:25\n",
      "\t 04/02/21 04:54:44PM EDT to 04/02/21 09:16:13PM EDT \t duration=04:21\n",
      "\n",
      "\t total time spent: 08:47\n",
      "\n",
      " 04/05/21 ------\n",
      "\t 04/05/21 07:41:20PM EDT to 04/05/21 08:09:14PM EDT \t duration=00:27\n",
      "\t 04/05/21 08:12:35PM EDT to 04/05/21 08:17:51PM EDT \t duration=00:05\n",
      "\t 04/05/21 08:21:03PM EDT to 04/05/21 08:26:54PM EDT \t duration=00:05\n",
      "\n",
      "\t total time spent: 00:39\n",
      "\n",
      " 04/06/21 ------\n",
      "\t 04/06/21 10:24:56AM EDT to 04/06/21 10:40:35AM EDT \t duration=00:15\n",
      "\t 04/06/21 11:58:53AM EDT to 04/06/21 12:07:18PM EDT \t duration=00:08\n",
      "\t 04/06/21 12:27:24PM EDT to 04/06/21 12:36:05PM EDT \t duration=00:08\n",
      "\t 04/06/21 04:49:50PM EDT to 04/06/21 04:57:02PM EDT \t duration=00:07\n",
      "\t 04/06/21 05:28:06PM EDT to 04/06/21 06:51:33PM EDT \t duration=01:23\n",
      "\t 04/06/21 07:00:16PM EDT to 04/06/21 07:48:19PM EDT \t duration=00:48\n",
      "\t 04/06/21 07:51:30PM EDT to 04/06/21 08:24:51PM EDT \t duration=00:33\n",
      "\t 04/06/21 08:28:38PM EDT to 04/06/21 09:18:43PM EDT \t duration=00:50\n",
      "\t 04/06/21 09:19:56PM EDT to 04/06/21 09:45:18PM EDT \t duration=00:25\n",
      "\t 04/06/21 10:55:59PM EDT to 04/06/21 11:28:10PM EDT \t duration=00:32\n",
      "\n",
      "\t total time spent: 05:12\n",
      "\n",
      " 04/08/21 ------\n",
      "\t 04/08/21 05:26:31PM EDT to 04/08/21 06:31:04PM EDT \t duration=01:04\n",
      "\t 04/08/21 06:33:26PM EDT to 04/08/21 07:16:11PM EDT \t duration=00:42\n",
      "\t 04/08/21 07:22:34PM EDT to 04/08/21 08:06:23PM EDT \t duration=00:43\n",
      "\t 04/08/21 08:07:35PM EDT to 04/08/21 09:57:21PM EDT \t duration=01:49\n",
      "\t 04/08/21 09:58:22PM EDT to 04/08/21 10:44:52PM EDT \t duration=00:46\n",
      "\n",
      "\t total time spent: 05:07\n",
      "\n",
      " 04/09/21 ------\n",
      "\t 04/09/21 10:22:05AM EDT to 04/09/21 10:43:30AM EDT \t duration=00:21\n",
      "\t 04/09/21 10:44:42AM EDT to 04/09/21 12:07:24PM EDT \t duration=01:22\n",
      "\t 04/09/21 01:12:42PM EDT to 04/09/21 01:58:52PM EDT \t duration=00:46\n",
      "\t 04/09/21 02:08:09PM EDT to 04/09/21 02:34:53PM EDT \t duration=00:26\n",
      "\t 04/09/21 02:47:09PM EDT to 04/09/21 03:53:47PM EDT \t duration=01:06\n",
      "\n",
      "\t total time spent: 04:03\n",
      "\n",
      " 04/12/21 ------\n",
      "\t 04/12/21 12:54:54PM EDT to 04/12/21 03:21:40PM EDT \t duration=02:26\n",
      "\t 04/12/21 03:23:54PM EDT to 04/12/21 05:14:55PM EDT \t duration=01:51\n",
      "\n",
      "\t total time spent: 04:17\n"
     ]
    }
   ],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "def tick_to_timestamp_ms_converter(tick_ref, timestamp_ms_ref):\n",
    "    return lambda tick_val: int(timestamp_ms_ref + tick_val - tick_ref)\n",
    "    \n",
    "def get_packets_by_range(start=\"01/01/01 01:01:01AM EST\", end=np.inf):\n",
    "    #accepts timestamp string or timestamp value in ms; returns ordered packets by servertimestamp\n",
    "    if type(start) == str: start = string_to_timestamp_ms(start)\n",
    "    if type(end) == str: end = string_to_timestamp_ms(end)\n",
    "    return sorted([x for x in data if x['serverTimestamp']>=start and x['serverTimestamp']<=end], key=lambda k: k['serverTimestamp'])\n",
    "\n",
    "def create_sessions(packets, thresh_sec=5):\n",
    "    #accepts list of sorted packets, breaks into sessions when: \n",
    "    # (1) we fail to see data from the server for >=thresh sec\n",
    "    # (2) we see a packet_tick_ms descend instead of ascend\n",
    "    \n",
    "    final_packet_array_of_arrays = []\n",
    "    current_session = []\n",
    "    current_timestamp_packets = [packets[0]]\n",
    "    \n",
    "    last_seen = packets[0]['serverTimestamp']\n",
    "    last_tick = packets[0]['packet_tick_ms']\n",
    "    \n",
    "    for packet in packets:\n",
    "        #duplicate packet\n",
    "        if packet['serverTimestamp'] == last_seen and packet['packet_tick_ms'] == last_tick:\n",
    "            pass \n",
    "        #packet that is at the same server time, but is increasing packet_tick from prev servertime\n",
    "        elif packet['serverTimestamp'] == last_seen and packet['packet_tick_ms'] > last_tick and packet['packet_tick_ms'] < last_tick + thresh_sec*1000:\n",
    "            current_timestamp_packets.append(packet)\n",
    "        #packet that is increased server time and packet tick\n",
    "        elif packet['serverTimestamp'] > last_seen and packet['packet_tick_ms'] > last_tick and packet['packet_tick_ms'] < last_tick + thresh_sec*1000:\n",
    "            current_session.extend(sorted(current_timestamp_packets, key=lambda k: k['packet_tick_ms']))\n",
    "            current_timestamp_packets = [packet]\n",
    "            last_seen = packet['serverTimestamp']\n",
    "            last_tick = current_session[-1]['packet_tick_ms']\n",
    "        #packet that has not increased servertime or not increased packettime within threshold \n",
    "        else:\n",
    "            #print('new session: ', last_seen, packet['serverTimestamp'], last_tick, packet['packet_tick_ms'])\n",
    "            if len(current_session) > 10:\n",
    "                final_packet_array_of_arrays.append(current_session)\n",
    "            current_session = []\n",
    "            current_timestamp_packets = [packet]\n",
    "            last_seen = packet['serverTimestamp']\n",
    "            last_tick = packet['packet_tick_ms']\n",
    " \n",
    "    if len(current_session) > 10:\n",
    "        final_packet_array_of_arrays.append(current_session)\n",
    "    \n",
    "    return final_packet_array_of_arrays\n",
    "    \n",
    "def filter_short_sessions(sessions, min_thresh=5):\n",
    "    return [session for session in sessions if (session[-1]['serverTimestamp'] - session[0]['serverTimestamp']) > min_thresh*60*1000]\n",
    "    \n",
    "def get_sessions_by_day(sessions, day=\"03/29\"):\n",
    "    return [session for session in sessions if timestamp_ms_to_string(session[0]['serverTimestamp']).startswith(day)]\n",
    "    \n",
    "def print_session_times(sessions):\n",
    "    last_day = ''\n",
    "    total_time = None\n",
    "    #separate days and print in groups by day\n",
    "    for i, session in enumerate(sessions):\n",
    "        session_day = timestamp_ms_to_string(session[0]['serverTimestamp'])[:8]\n",
    "        if session_day != last_day:\n",
    "            if total_time is None: total_time = 0\n",
    "            else:\n",
    "                print('\\n\\t total time spent: %02d:%02d' % (total_time // 60, total_time % 60))\n",
    "                total_time = 0\n",
    "            \n",
    "            last_day = session_day\n",
    "            print('\\n',session_day,'------')\n",
    "        \n",
    "        duration = (session[-1]['serverTimestamp'] - session[0]['serverTimestamp'])/(60*1000)\n",
    "        duration_string = \"duration=%02d:%02d\" % (duration // 60, duration % 60)\n",
    "        total_time += duration\n",
    "                      \n",
    "        print('\\t', timestamp_ms_to_string(session[0]['serverTimestamp']), 'to', timestamp_ms_to_string(session[-1]['serverTimestamp']), '\\t', duration_string)\n",
    "    \n",
    "    print('\\n\\t total time spent: %02d:%02d' % (total_time // 60, total_time % 60))\n",
    "                \n",
    "\n",
    "with open('./cleaned_data/captivateFiltered.json') as f: data = json.load(f)\n",
    "\n",
    "#grab packets from a certain range (accepts timestamp ints in ms or datestrings of this form)\n",
    "sorted_data = get_packets_by_range(start=\"03/24/21 12:00:00PM EST\")\n",
    "\n",
    "#sort them into sessions and see how many sessions we got\n",
    "data_by_session = create_sessions(sorted_data)\n",
    "total_num_sessions = len(data_by_session)\n",
    "\n",
    "#filter out the sessions that are less than 5 min\n",
    "data_by_session = filter_short_sessions(data_by_session)\n",
    "print(len(data_by_session),'sessions found. (%d short sessions filtered.)' % (total_num_sessions - len(data_by_session)))\n",
    "\n",
    "#print the session information by day\n",
    "print_session_times(data_by_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded all the glasses data in and separated them into sessions, we'll go through\n",
    "and push them to each folder by day, and update the video_metadata as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david_2\n",
      "{'date': '03/25', 'vid_start': '03/25/21 01:41:01PM EDT', 'start_error_sec': 60, 'glasses_session_times': [['03/25/21 02:25:13PM EDT', '03/25/21 05:55:40PM EDT'], ['03/25/21 06:11:11PM EDT', '03/25/21 08:32:30PM EDT']], 'glasses_session_durations_sec': [210.46036666666666, 141.32051666666666], 'glasses_sessions_total_duration_string': '05:51', 'glasses_session_durations_min': [210.46036666666666, 141.32051666666666]}\n",
      "patrick_1\n",
      "{'date': '03/29', 'vid_start': '', 'start_error_sec': 60, 'glasses_session_times': [['03/29/21 03:28:19PM EDT', '03/29/21 04:07:45PM EDT'], ['03/29/21 04:10:38PM EDT', '03/29/21 05:48:17PM EDT'], ['03/29/21 05:51:01PM EDT', '03/29/21 06:49:15PM EDT'], ['03/29/21 06:49:41PM EDT', '03/29/21 07:27:35PM EDT'], ['03/29/21 07:29:10PM EDT', '03/29/21 08:01:04PM EDT']], 'glasses_session_durations_sec': [39.43695, 97.6538, 58.235866666666666, 37.906083333333335, 31.891], 'glasses_sessions_total_duration_string': '04:25', 'glasses_session_durations_min': [39.43695, 97.6538, 58.235866666666666, 37.906083333333335, 31.891]}\n",
      "patrick_2\n",
      "{'date': '04/01', 'vid_start': '', 'start_error_sec': 60, 'glasses_session_times': [['04/01/21 02:35:30PM EDT', '04/01/21 02:45:05PM EDT'], ['04/01/21 03:07:10PM EDT', '04/01/21 03:23:42PM EDT'], ['04/01/21 03:24:54PM EDT', '04/01/21 03:44:40PM EDT'], ['04/01/21 04:02:56PM EDT', '04/01/21 05:12:18PM EDT'], ['04/01/21 05:13:58PM EDT', '04/01/21 05:47:34PM EDT'], ['04/01/21 05:47:39PM EDT', '04/01/21 07:38:31PM EDT'], ['04/01/21 07:39:42PM EDT', '04/01/21 08:04:11PM EDT']], 'glasses_session_durations_sec': [9.579833333333333, 16.530266666666666, 19.772366666666667, 69.37225, 33.604083333333335, 110.85515, 24.474916666666665], 'glasses_sessions_total_duration_string': '04:44', 'glasses_session_durations_min': [9.579833333333333, 16.530266666666666, 19.772366666666667, 69.37225, 33.604083333333335, 110.85515, 24.474916666666665]}\n",
      "beata_1\n",
      "{'date': '04/02', 'vid_start': '04/02/21 12:16:30PM EDT', 'start_error_sec': 60, 'glasses_session_times': [['04/02/21 12:13:25PM EDT', '04/02/21 04:39:00PM EDT'], ['04/02/21 04:54:44PM EDT', '04/02/21 09:16:13PM EDT']], 'glasses_session_durations_sec': [265.59208333333333, 261.47523333333334], 'glasses_sessions_total_duration_string': '08:47', 'glasses_session_durations_min': [265.59208333333333, 261.47523333333334]}\n",
      "david_3a\n",
      "{'date': '04/06', 'vid_start': '04/06/21 04:48:22PM EDT', 'start_error_sec': 15, 'glasses_session_times': [['04/06/21 10:24:56AM EDT', '04/06/21 10:40:35AM EDT'], ['04/06/21 11:58:53AM EDT', '04/06/21 12:07:18PM EDT'], ['04/06/21 12:27:24PM EDT', '04/06/21 12:36:05PM EDT'], ['04/06/21 04:49:50PM EDT', '04/06/21 04:57:02PM EDT'], ['04/06/21 05:28:06PM EDT', '04/06/21 06:51:33PM EDT'], ['04/06/21 07:00:16PM EDT', '04/06/21 07:48:19PM EDT'], ['04/06/21 07:51:30PM EDT', '04/06/21 08:24:51PM EDT'], ['04/06/21 08:28:38PM EDT', '04/06/21 09:18:43PM EDT'], ['04/06/21 09:19:56PM EDT', '04/06/21 09:45:18PM EDT'], ['04/06/21 10:55:59PM EDT', '04/06/21 11:28:10PM EDT']], 'glasses_session_durations_sec': [15.65245, 8.4199, 8.679533333333334, 7.19655, 83.4524, 48.05713333333333, 33.33423333333333, 50.08598333333333, 25.37265, 32.187], 'glasses_sessions_total_duration_string': '05:12', 'glasses_session_durations_min': [15.65245, 8.4199, 8.679533333333334, 7.19655, 83.4524, 48.05713333333333, 33.33423333333333, 50.08598333333333, 25.37265, 32.187]}\n",
      "david_3b\n",
      "{'date': '04/06', 'vid_start': '04/06/21 05:28:57PM EDT', 'start_error_sec': 15, 'glasses_session_times': [['04/06/21 10:24:56AM EDT', '04/06/21 10:40:35AM EDT'], ['04/06/21 11:58:53AM EDT', '04/06/21 12:07:18PM EDT'], ['04/06/21 12:27:24PM EDT', '04/06/21 12:36:05PM EDT'], ['04/06/21 04:49:50PM EDT', '04/06/21 04:57:02PM EDT'], ['04/06/21 05:28:06PM EDT', '04/06/21 06:51:33PM EDT'], ['04/06/21 07:00:16PM EDT', '04/06/21 07:48:19PM EDT'], ['04/06/21 07:51:30PM EDT', '04/06/21 08:24:51PM EDT'], ['04/06/21 08:28:38PM EDT', '04/06/21 09:18:43PM EDT'], ['04/06/21 09:19:56PM EDT', '04/06/21 09:45:18PM EDT'], ['04/06/21 10:55:59PM EDT', '04/06/21 11:28:10PM EDT']], 'glasses_session_durations_sec': [15.65245, 8.4199, 8.679533333333334, 7.19655, 83.4524, 48.05713333333333, 33.33423333333333, 50.08598333333333, 25.37265, 32.187], 'glasses_sessions_total_duration_string': '05:12', 'glasses_session_durations_min': [15.65245, 8.4199, 8.679533333333334, 7.19655, 83.4524, 48.05713333333333, 33.33423333333333, 50.08598333333333, 25.37265, 32.187]}\n",
      "irmandy_1\n",
      "{'date': '04/08', 'vid_start': '04/08/21 05:30:56PM EDT', 'start_error_sec': 15, 'glasses_session_times': [['04/08/21 05:26:31PM EDT', '04/08/21 06:31:04PM EDT'], ['04/08/21 06:33:26PM EDT', '04/08/21 07:16:11PM EDT'], ['04/08/21 07:22:34PM EDT', '04/08/21 08:06:23PM EDT'], ['04/08/21 08:07:35PM EDT', '04/08/21 09:57:21PM EDT'], ['04/08/21 09:58:22PM EDT', '04/08/21 10:44:52PM EDT']], 'glasses_session_durations_sec': [64.55098333333333, 42.7433, 43.8198, 109.75191666666667, 46.49978333333333], 'glasses_sessions_total_duration_string': '05:07', 'glasses_session_durations_min': [64.55098333333333, 42.7433, 43.8198, 109.75191666666667, 46.49978333333333]}\n",
      "juliana_1\n",
      "{'date': '04/09', 'vid_start': '04/09/21 10:21:33AM EDT', 'start_error_sec': 15, 'glasses_session_times': [['04/09/21 10:22:05AM EDT', '04/09/21 10:43:30AM EDT'], ['04/09/21 10:44:42AM EDT', '04/09/21 12:07:24PM EDT'], ['04/09/21 01:12:42PM EDT', '04/09/21 01:58:52PM EDT'], ['04/09/21 02:08:09PM EDT', '04/09/21 02:34:53PM EDT'], ['04/09/21 02:47:09PM EDT', '04/09/21 03:53:47PM EDT']], 'glasses_session_durations_sec': [21.416833333333333, 82.70546666666667, 46.157916666666665, 26.728783333333332, 66.6337], 'glasses_sessions_total_duration_string': '04:03', 'glasses_session_durations_min': [21.416833333333333, 82.70546666666667, 46.157916666666665, 26.728783333333332, 66.6337]}\n"
     ]
    }
   ],
   "source": [
    "for folder in video_metadata:\n",
    "    \n",
    "    #get sesssions for day that is represented by the folder\n",
    "    sessions_to_folder = get_sessions_by_day(data_by_session, day=video_metadata[folder]['date'])\n",
    "    \n",
    "    #collect and append metadata about session start/end timestamps and durations\n",
    "    video_metadata[folder]['glasses_session_times'] = \\\n",
    "        [[timestamp_ms_to_string(session[0]['serverTimestamp']), \n",
    "          timestamp_ms_to_string(session[-1]['serverTimestamp'])] for session in sessions_to_folder]\n",
    "    \n",
    "    video_metadata[folder]['glasses_session_durations_min'] = \\\n",
    "        [(session[-1]['serverTimestamp'] - session[0]['serverTimestamp'])/(60*1000) for session in sessions_to_folder]\n",
    "                                                                      \n",
    "    total_dur = sum(video_metadata[folder]['glasses_session_durations_min'])\n",
    "    video_metadata[folder]['glasses_sessions_total_duration_string'] = \"%02d:%02d\" % (total_dur // 60, total_dur % 60)\n",
    "        \n",
    "    #save sessions in folder as pickle\n",
    "    pickle.dump( sessions_to_folder, open( \"./cleaned_data/\" + folder + \"/glasses_sessions.p\", \"wb\" ) )\n",
    "\n",
    "#remove this giant object from memory\n",
    "del data\n",
    "\n",
    "for folder in video_metadata:\n",
    "    print(folder)\n",
    "    print(video_metadata[folder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**A Note from Patrick:**\n",
    "\n",
    "The csv is rather large so unless you have enough RAM, you can upload it in chunks or upload individual columns (ref: https://stackoverflow.com/questions/25962114/how-do-i-read-a-large-csv-file-with-pandas)\n",
    "\n",
    " \n",
    "\n",
    "As for the data, the full details of the format is explained here but in summary, youâ€™re going to want to look at columns AU45_r and AU45_l for blinks and pose_Rx, pose_Ry, and pose_Rz for head pose (these are radians).\n",
    "\n",
    "----\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate our video_min.csv files so we don't have to work with GB csvs\n",
    "Now we want to run through all folders, if there is a 'video.csv' file and no 'video_min.csv', we\n",
    "will create 'video_min.csv' with only a few features so we have something we can load into memory easily.\n",
    "\n",
    "----\n",
    "\n",
    "#### Openface data: http://lukacreu.blogspot.com/2017/11/week-8-openface-ouput.html\n",
    "\n",
    "- frame, timestamp in sec, confidence in track, success decision on track\n",
    "- gaze: direction vector in world coord, normalized\n",
    "- gaze_angle: direction in radians in world coord\n",
    "- eye_lmk: pixel loc of landmarks in 2D\n",
    "- pose_T: head location relative to camera, in mm\n",
    "- pose_R: head rotation in radian, left hand positive (pitch/roll/yaw) prob in world coord, can be camera coord (world_coord flag)\n",
    "- lowercase x/y: pixel landmarks in 2D\n",
    "- uppercase x/y/z: mm landmarks in 3D\n",
    "- p: point distribution model of face shape\n",
    "\n",
    "#### AU mapping: https://imotions.com/blog/facial-action-coding-system/\n",
    "\n",
    "- AU<XX>_r is intensity from 0-5\n",
    "- AU<XX>_c is presence (0 or 1)\n",
    "\n",
    "ones of interest may be:\n",
    "- AU 41-45 (eyelid droop, slit, closed, squint, blink)\n",
    "- AU 1-7 (1-4 eyebrows, 5 raising eyelids in surprise, 6 cheeks when smile, 7 ~squint)\n",
    "- AU 61-64 (eye movements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print possible columns to include in video_min.csv\n",
    "'''\n",
    "columns_to_choose_from = pd.read_csv('./cleaned_data/david_1/video.csv', index_col=0, nrows=0).columns.tolist()\n",
    "for c in columns_to_choose_from:\n",
    "    print(c, end='\\t')\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found video_min.csv for david_2.\n",
      "no video_min.csv for patrick_1, trying min command...\n",
      "min command failed! probably no video.csv in folder!\n",
      "found video_min.csv for patrick_2.\n",
      "found video_min.csv for beata_1.\n",
      "no video_min.csv for david_3a, trying min command...\n",
      "min command failed! probably no video.csv in folder!\n",
      "no video_min.csv for david_3b, trying min command...\n",
      "min command failed! probably no video.csv in folder!\n",
      "found video_min.csv for irmandy_1.\n",
      "found video_min.csv for juliana_1.\n"
     ]
    }
   ],
   "source": [
    "#it will save only these columns:\n",
    "columns_to_save = ['timestamp', 'AU45_r', 'AU45_c', \n",
    "                   'pose_Rx', 'pose_Ry', 'pose_Rz', \n",
    "                   'pose_Tx', 'pose_Ty', 'pose_Tz']\n",
    "    \n",
    "def minimal_csv_file(csv_filename):\n",
    "    chunksize = 3600 #one min at 60fps\n",
    "    video_df = pd.DataFrame() \n",
    "\n",
    "    with pd.read_csv(csv_filename, delimiter=', ', engine='python', chunksize=chunksize) as reader:\n",
    "        for chunk in reader:\n",
    "            print('.', end='')\n",
    "            chunk = chunk[columns_to_save] \n",
    "            video_df = pd.concat([video_df, chunk])\n",
    "    \n",
    "    video_df.to_csv(csv_filename[:-4] + '_min.csv')\n",
    "    print('DONE!')\n",
    "\n",
    "for folder in video_metadata:\n",
    "    folder_path = './cleaned_data/' + folder \n",
    "    if not pathlib.Path(folder_path + '/video_min.csv').is_file():\n",
    "        try:\n",
    "            print('no video_min.csv for ' + folder + ', trying min command...')\n",
    "            minimal_csv_file(folder_path + '/video.csv')\n",
    "        except:\n",
    "            print('min command failed! probably no video.csv in folder!')\n",
    "    else:\n",
    "        print('found video_min.csv for ' + folder + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Generating Metadata \n",
    "Now that we have video_min.csv files to work with, will run through and grab durations\n",
    "from our video_min files and add it to our metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no min_csv file for patrick_1\n",
      "no min_csv file for david_3a\n",
      "no min_csv file for david_3b\n"
     ]
    }
   ],
   "source": [
    "for folder in video_metadata:\n",
    "    min_csv_path = './cleaned_data/' + folder + '/video_min.csv' \n",
    "    try:\n",
    "        video_metadata[folder]['vid_duration_sec'] = pd.read_csv(min_csv_path)['timestamp'].iloc[-1]\n",
    "    except:\n",
    "        print('no min_csv file for ' + folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can print out a summary nice and easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "david_2 on 03/25\n",
      "05:51 hr of glassess data starting at 03/25/21 02:25:13PM EDT\n",
      "04:30 hr of  vid starting at 03/25/21 01:41:01PM EDT\n",
      "====================\n",
      "patrick_1 on 03/29\n",
      "04:25 hr of glassess data starting at 03/29/21 03:28:19PM EDT\n",
      "failed to load video analysis file\n",
      "====================\n",
      "patrick_2 on 04/01\n",
      "04:44 hr of glassess data starting at 04/01/21 02:35:30PM EDT\n",
      "04:46 hr of  vid starting at \n",
      "====================\n",
      "beata_1 on 04/02\n",
      "08:47 hr of glassess data starting at 04/02/21 12:13:25PM EDT\n",
      "08:07 hr of  vid starting at 04/02/21 12:16:30PM EDT\n",
      "====================\n",
      "david_3a on 04/06\n",
      "05:12 hr of glassess data starting at 04/06/21 10:24:56AM EDT\n",
      "failed to load video analysis file\n",
      "====================\n",
      "david_3b on 04/06\n",
      "05:12 hr of glassess data starting at 04/06/21 10:24:56AM EDT\n",
      "failed to load video analysis file\n",
      "====================\n",
      "irmandy_1 on 04/08\n",
      "05:07 hr of glassess data starting at 04/08/21 05:26:31PM EDT\n",
      "00:14 hr of  vid starting at 04/08/21 05:30:56PM EDT\n",
      "====================\n",
      "juliana_1 on 04/09\n",
      "04:03 hr of glassess data starting at 04/09/21 10:22:05AM EDT\n",
      "05:58 hr of  vid starting at 04/09/21 10:21:33AM EDT\n"
     ]
    }
   ],
   "source": [
    "for folder in video_metadata:\n",
    "    try:\n",
    "        print('='*20)\n",
    "        print(folder, 'on', video_metadata[folder]['date'])\n",
    "        print(video_metadata[folder]['glasses_sessions_total_duration_string'], \\\n",
    "               'hr of glassess data starting at', \\\n",
    "               video_metadata[folder]['glasses_session_times'][0][0])\n",
    "        print('%02d:%02d hr of  vid starting at ' % \n",
    "              (video_metadata[folder]['vid_duration_sec'] / 60 // 60, \n",
    "               video_metadata[folder]['vid_duration_sec'] / 60 % 60 )  + \\\n",
    "               video_metadata[folder]['vid_start'])\n",
    "    except:\n",
    "        print('failed to load video analysis file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( video_metadata, open( \"./cleaned_data/metadata.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Sessions\n",
    "\n",
    "now lets go through and make our sessions into easy to work with csv files that we can import\n",
    "for our blink and position analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blink_timestamp_to_array(timestamp_ms, num_points=100, Fs=1000):\n",
    "    T_ms = (1000./Fs)\n",
    "    start_time = timestamp_ms - ((num_points-1) * T_ms)\n",
    "    return [int(start_time + T_ms*i) for i in range(0, num_points)]\n",
    "    \n",
    "def blink_string_to_array(string):\n",
    "    return struct.unpack('<200B', string.encode('UTF-16-LE'))[::2] #'UTF-16-LE'\n",
    "    \n",
    "\n",
    "## GET SPECIFIC DATA TYPE\n",
    "def get_blink_data(data):\n",
    "    #remove any packets without blinkdata\n",
    "    data = [d for d in data if len(d['blink_data']) == 100]\n",
    "    \n",
    "    if not len(data): \n",
    "        print('NO VALID BLINK DATA DETECTED')\n",
    "        return [],[]\n",
    "    \n",
    "    #associate first packet timestamp with tick to give us a tick to timestamp converter function\n",
    "    tick_to_timestamp_fn = tick_to_timestamp_ms_converter(data[0]['blink_tick_ms'], data[0]['serverTimestamp'])\n",
    "    \n",
    "    #grab data and associated timestamp/ticks\n",
    "    times = [blink_timestamp_to_array(tick_to_timestamp_fn(d['blink_tick_ms'])) for d in data]\n",
    "    blink_data = [blink_string_to_array(d['blink_data']) for d in data]\n",
    "\n",
    "    return flatten(times), flatten(blink_data)\n",
    "\n",
    "def get_temp_data(data):\n",
    "    #remove any packets without tempdata\n",
    "    data = [d for d in data if (d['temple_tick_ms'] != 0 and d['nose_tick_ms'] != 0)]\n",
    "    \n",
    "    if not len(data): \n",
    "        print('NO VALID TEMP DATA DETECTED')\n",
    "        return [],[],[],[]\n",
    "    \n",
    "    #associate first packet timestamp with tick to give us a tick to timestamp converter function\n",
    "    tick_to_timestamp_fn = tick_to_timestamp_ms_converter(data[0]['temple_tick_ms'], data[0]['serverTimestamp'])\n",
    "    \n",
    "    #grab data and associated timestamp/ticks\n",
    "    temple_times = [tick_to_timestamp_fn(d['temple_tick_ms']) for d in data]\n",
    "    temple_data = [d['temple_temp'] for d in data]\n",
    "    nose_times = [tick_to_timestamp_fn(d['nose_tick_ms']) for d in data]\n",
    "    nose_data = [d['nose_temp'] for d in data]\n",
    "    \n",
    "    return temple_times, temple_data, nose_times, nose_data \n",
    "\n",
    "def get_quat_data(data):\n",
    "    #remove any packets without quatdata\n",
    "    data = [d for d in data if (d['rot_tick_ms'] != 0)]\n",
    "    \n",
    "    if not len(data): \n",
    "        print('NO VALID QUAT DATA DETECTED')\n",
    "        return [],[],[],[],[],[]\n",
    "    \n",
    "    #associate first packet timestamp with tick to give us a tick to timestamp converter function\n",
    "    tick_to_timestamp_fn = tick_to_timestamp_ms_converter(data[0]['rot_tick_ms'], data[0]['serverTimestamp'])\n",
    "    \n",
    "    #grab data and associated timestamp/ticks\n",
    "    times = [tick_to_timestamp_fn(d['rot_tick_ms']) for d in data]\n",
    "    I_data = [d['quatI'] for d in data]\n",
    "    J_data = [d['quatJ'] for d in data]\n",
    "    K_data = [d['quatK'] for d in data]\n",
    "    real_data = [d['quatReal'] for d in data]\n",
    "    accuracy_data = [d['quatRadianAccuracy'] for d in data]\n",
    "    \n",
    "    return times, I_data, J_data, K_data, real_data, accuracy_data  \n",
    "\n",
    "def get_pos_data(data):\n",
    "    #remove any packets without posdata\n",
    "    data = [d for d in data if (d['tick_ms_pos'] != 0)]\n",
    "    \n",
    "    if not len(data): \n",
    "        print('NO VALID POS DATA DETECTED')\n",
    "        return [],[],[],[],[]\n",
    "    \n",
    "    #associate first packet timestamp with tick to give us a tick to timestamp converter function\n",
    "    tick_to_timestamp_fn = tick_to_timestamp_ms_converter(data[0]['tick_ms_pos'], data[0]['serverTimestamp'])\n",
    "    \n",
    "    #grab data and associated timestamp/ticks\n",
    "    times = [tick_to_timestamp_fn(d['tick_ms_pos']) for d in data]\n",
    "    x_data = [d['pos_x'] for d in data]\n",
    "    y_data = [d['pos_y'] for d in data]\n",
    "    z_data = [d['pos_z'] for d in data]\n",
    "    accuracy_data = [d['pos_accuracy'] for d in data]\n",
    "    \n",
    "    return times, x_data, y_data, z_data, accuracy_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on david_2, 2 sessions...\n",
      "working on patrick_1, 5 sessions...\n",
      "working on patrick_2, 7 sessions...\n",
      "NO VALID QUAT DATA DETECTED\n",
      "working on beata_1, 2 sessions...\n",
      "working on david_3a, 10 sessions...\n",
      "NO VALID TEMP DATA DETECTED\n",
      "working on david_3b, 10 sessions...\n",
      "NO VALID TEMP DATA DETECTED\n",
      "working on irmandy_1, 5 sessions...\n",
      "working on juliana_1, 5 sessions...\n"
     ]
    }
   ],
   "source": [
    "for folder in video_metadata:\n",
    "    path_base = './cleaned_data/' + folder \n",
    "    \n",
    "    sessions = pickle.load(open(path_base + '/glasses_sessions.p', 'rb'))\n",
    "    print('working on ' + folder + ',', len(sessions), 'sessions...')\n",
    "\n",
    "    b_sessions = [get_blink_data(s) for s in sessions]\n",
    "    b_sessions = [pd.DataFrame({'timestamp_ms':s[0], 'value':s[1]}) for s in b_sessions]\n",
    "    pickle.dump( b_sessions, open( path_base + \"/sessions_blink.p\", \"wb\" ) )\n",
    "\n",
    "    t_sessions = [get_temp_data(s)  for s in sessions]\n",
    "    t_sessions = [pd.DataFrame({'timestamp_tmpl_ms':s[0], 'tmpl_temp':s[1],\n",
    "                            'timestamp_nose_ms':s[2], 'nose_temp':s[3],}) for s in t_sessions]\n",
    "    pickle.dump( t_sessions, open( path_base + \"/sessions_temps.p\", \"wb\" ) )\n",
    "\n",
    "    q_sessions = [get_quat_data(s)  for s in sessions]\n",
    "    q_sessions = [pd.DataFrame({'timestamp_ms':s[0], 'quatI':s[1], 'quatJ':s[2], 'quatK':s[3],\n",
    "                            'quatReal':s[4], 'quatRadianAccuracy':s[5]}) for s in q_sessions]\n",
    "    pickle.dump( q_sessions, open( path_base + \"/sessions_accel.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate Video Data\n",
    "\n",
    "take our openface features (min) and our eyeratio features, strap on the estimated timestamp based on our estimated starttime, and consolidate into one df to rule them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gopro_timestamp_fnc(start_timestamp=\"01/01/01 01:01:00AM EST\"):\n",
    "    #start_timestamp can be string of form above, or timestamp in ms\n",
    "    if type(start_timestamp) == str:\n",
    "        start_timestamp = datetime.strptime(start_timestamp, '%m/%d/%y %I:%M:%S%p %Z').timestamp()*1000\n",
    "    \n",
    "    def get_gopro_timestamp_ms(offset_ms):\n",
    "        return int(start_timestamp + offset_ms)\n",
    "    \n",
    "    return get_gopro_timestamp_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on patrick_2video data...\n",
      "   eye_ratio  timestamp\n",
      "0   0.001489  16.683426\n",
      "1   0.001445  33.366851\n",
      "2   0.001373  50.050277\n",
      "3   0.001240  66.733702\n",
      "4   0.001435  83.417128\n",
      "time data '' does not match format '%m/%d/%y %I:%M:%S%p %Z'\n",
      "FAILED: prob no eyeratio or video_min file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "for folder in video_metadata:\n",
    "    print('working on ' + folder + 'video data...')\n",
    "\n",
    "    try:\n",
    "        #load video eyeratio, 60 Hz (16 ms between samples)\n",
    "        df_er = pd.read_csv('./cleaned_data/' + folder + '/video_eyeratio_final.csv', index_col=0)\n",
    "    \n",
    "        #load video openface features, 60 Hz (16 ms between samples)\n",
    "        df_of = pd.read_csv('./cleaned_data/' + folder + '/video_min.csv', index_col=0)\n",
    "      \n",
    "        gopro_est_timestamp_ms = make_gopro_timestamp_fnc(video_metadata[folder]['vid_start'])\n",
    "\n",
    "        #df_er is in ms already (because I generated it)\n",
    "        df_er['tick_ms'] = df_er['timestamp'].apply(lambda v: round(v-df_er['timestamp'].iloc[0]))\n",
    "        df_er['est_timestamp_ms'] = df_er['tick_ms'].apply(gopro_est_timestamp_ms)\n",
    "        del df_er['timestamp']\n",
    "\n",
    "        #df_of is in partial secs\n",
    "        df_of['tick_ms'] = df_of['timestamp'].apply(lambda v: round(v*1000.))\n",
    "        df_of['est_timestamp_ms'] = df_of['tick_ms'].apply(gopro_est_timestamp_ms)\n",
    "        del df_of['timestamp']\n",
    "\n",
    "        #consolidate based on tick_ms\n",
    "        final_df = pd.merge(df_er, df_of, on=[\"tick_ms\", \"est_timestamp_ms\"])\n",
    "\n",
    "        if (len(final_df) != min(len(df_er),len(df_of))): print('MERGE FAILED AHHHHHHHHH')\n",
    "        else: final_df.to_csv('./cleaned_data/' + folder + '/video_consolidated.csv')\n",
    "    \n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        print('FAILED: prob no eyeratio or video_min file, or no go-pro init timestamp!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "all the data is now clean: we can load it as follows:\n",
    "\n",
    "get the metadata about the folders:\n",
    "\n",
    "```\n",
    "metadata = pickle.load(open('./cleaned_data/metadata.p', 'rb'))\n",
    "print('Please Choose a Session to Work With:')\n",
    "for k in metadata: print(k, end=',  ')\n",
    "```\n",
    "\n",
    "pick a session and load it (video data is in a pandas DF, glasses data is an array of DFs\n",
    "broken up by session):\n",
    "\n",
    "```\n",
    "SESSION = 'david_1'\n",
    "\n",
    "#load video data (eyeratio and openface features), 60 Hz (16 ms between samples)\n",
    "df_vid = pd.read_csv('./cleaned_data/' + SESSION + '/video_consolidated.csv', index_col=0)\n",
    "\n",
    "#load glasses blink data, 1kHz (1 ms between samples)\n",
    "blink_sess = pickle.load(open('./cleaned_data/' + SESSION + '/sessions_blink.p', 'rb'))\n",
    "\n",
    "#load glasses accel data, 10 Hz (100 ms between samples)\n",
    "accel_sess = pickle.load(open('./cleaned_data/' + SESSION + '/sessions_accel.p', 'rb'))\n",
    "\n",
    "#load glasses temp data, 10 Hz (100 ms between samples)\n",
    "temps_sess = pickle.load(open('./cleaned_data/' + SESSION + '/sessions_temps.p', 'rb'))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Go forth and rejoice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
